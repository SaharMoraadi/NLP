{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "   \n",
    "### Section 1: Data Preparation\n",
    "\n",
    "-   Statements, Libraries, functions and tools\n",
    "-   Collecting data\n",
    "-   Converting to pandas dataframe\n",
    "-   General Preproccessing\n",
    "\n",
    "\n",
    "### Section 2: Preprocessing:Gensim - model:Random Forest\n",
    "\n",
    "\n",
    "-   Cleaning and tokenization dataset by Gensim preprossing package.\n",
    "-   Vectorization : word2vec embedding \n",
    "-   Fit Ranodom Forest Classifier\n",
    "-   Evaluation the model\n",
    " \n",
    "\n",
    "\n",
    "### Section 3: Built in a Basic RNN\n",
    "\n",
    "-   Tokenization dataset by keras \n",
    "-   Pad the sequences with the same length\n",
    "-   Fit the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook trains couple of models to classify tweets as a hate speech or not based on the text of the tweets.\n",
    "\n",
    "The tweets are primarily in English Language\n",
    "we used the [tweets_hate_speech_detection](https://huggingface.co/datasets/tweets_hate_speech_detection#source-data) from the Hugging face.\n",
    "\n",
    "label : \n",
    "- it is a hate speech, \n",
    "- not a hate speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.  Statements, Libraries, functions and tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# nltk:\n",
    "import re  # use if tor tokenizing\n",
    "import string\n",
    "\n",
    "#sklearn\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Model Evaluation:\n",
    "from sklearn.metrics import precision_score,recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#gensim\n",
    "import gensim\n",
    "\n",
    "# tensorflow:\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *   # layers, losses, preprocessing\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import keras.backend as k\n",
    "from keras.layers import Dense,Embedding, LSTM\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0-rc1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Collecting data from Hugging Face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (C:\\Users\\smora\\.cache\\huggingface\\datasets\\tweets_hate_speech_detection\\default\\0.0.0\\c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n"
     ]
    }
   ],
   "source": [
    "from datasets import list_datasets,load_dataset\n",
    "from pprint import pprint\n",
    "\n",
    "dataset = load_dataset(\"tweets_hate_speech_detection\" , split = \"train\"  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [pprint](https://docs.python.org/3/library/pprint.html) module provides a capability to “pretty-print”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['label', 'tweet'],\n",
      "    num_rows: 31962\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful codes for sophisticated datasets.\n",
    "#print(\"Column names \", dataset.column_names)\n",
    "#print(\"Number of columns :\", dataset.num_columns)\n",
    "#print(\"Number of rows : \", dataset.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First example: \n",
      "\n",
      "{'label': 0,\n",
      " 'tweet': '@user when a father is dysfunctional and is so selfish he drags his '\n",
      "          'kids into his dysfunction.   #run'}\n"
     ]
    }
   ],
   "source": [
    "print(\"First example: \\n\")\n",
    "pprint(dataset[0]) # print the first sample as a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 .Converting to pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ððððð¦ð¦ð¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  \\\n",
       "0      0   \n",
       "1      0   \n",
       "2      0   \n",
       "3      0   \n",
       "4      0   \n",
       "\n",
       "                                                                                                 tweet  \n",
       "0  @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.  ...  \n",
       "1  @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.  ...  \n",
       "2                                                                                  bihday your majesty  \n",
       "3               #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦    \n",
       "4                                                               factsguide: society now    #motivation  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth',100)\n",
    "#imdb_train.set_format(\"pandas\")\n",
    "##df_imdb_train = imdb_train[:]\n",
    "\n",
    "#imdb_test.set_format(\"pandas\")\n",
    "#df_imdb_test = imdb_test[:]\n",
    "\n",
    "#df_imdb_train.head()\n",
    "\n",
    "dataset.set_format(\"pandas\")\n",
    "tweet_df = dataset[:]\n",
    "tweet_df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labels : 0 means Negative and 1 means Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = tweet_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 . General Preproccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = nltk.corpus.stopwords.words('english')\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "   # text_noNum=\"\".join([char for char in text_noPunc if char not in '123456789'])\n",
    "\n",
    "def cleaned_text (text):\n",
    "    text_noPunc=\"\".join([char.lower() for char in text if char not in string.punctuation])\n",
    "    text_tokenized = re.split('\\W+' , text_noPunc)\n",
    "    text_noStopWords = [ word for word in text_tokenized if word not in stopWords]\n",
    "    text_lemmatized =  [wn.lemmatize(word) for word in text_noStopWords]\n",
    "    return text_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.  ...</td>\n",
       "      <td>[user, father, dysfunctional, selfish, drag, kid, dysfunction, run]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.  ...</td>\n",
       "      <td>[user, user, thanks, lyft, credit, cant, use, cause, dont, offer, wheelchair, van, pdx, disapoin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>[bihday, majesty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ððððð¦ð¦ð¦</td>\n",
       "      <td>[model, love, u, take, u, time, urð, ð, ð, ð, ð, ð, ð, ð, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>[factsguide, society, motivation]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  \\\n",
       "0      0   \n",
       "1      0   \n",
       "2      0   \n",
       "3      0   \n",
       "4      0   \n",
       "\n",
       "                                                                                                 tweet  \\\n",
       "0  @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.  ...   \n",
       "1  @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.  ...   \n",
       "2                                                                                  bihday your majesty   \n",
       "3               #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦     \n",
       "4                                                               factsguide: society now    #motivation   \n",
       "\n",
       "                                                                                           clean_tweet  \n",
       "0                                  [user, father, dysfunctional, selfish, drag, kid, dysfunction, run]  \n",
       "1  [user, user, thanks, lyft, credit, cant, use, cause, dont, offer, wheelchair, van, pdx, disapoin...  \n",
       "2                                                                                    [bihday, majesty]  \n",
       "3                                          [model, love, u, take, u, time, urð, ð, ð, ð, ð, ð, ð, ð, ]  \n",
       "4                                                                    [factsguide, society, motivation]  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet['clean_tweet'] = tweet['tweet'].apply ( lambda x: cleaned_text(x))\n",
    "tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = train_test_split(tweet, test_size  = 0.2 , random_state=142)\n",
    "X_train_tweet = train['clean_tweet']\n",
    "y_train_tweet = train['label']\n",
    "X_test_tweet = test['clean_tweet']\n",
    "y_test_tweet = test['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing:Gensim - model:Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim (Generate Similar) is an open-source library implemented in Python designed for natural language processing and unsupervised modelling.\n",
    "            Handling large text files without loading in memory is of one the significant advantages of the Gensim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 .Cleaning and tokenization dataset by Gensim preprossing package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_gensim = tweet_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.  ...</td>\n",
       "      <td>[user, when, father, is, dysfunctional, and, is, so, selfish, he, drags, his, kids, into, his, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.  ...</td>\n",
       "      <td>[user, user, thanks, for, lyft, credit, can, use, cause, they, don, offer, wheelchair, vans, in,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ððððð¦ð¦ð¦</td>\n",
       "      <td>[model, love, take, with, all, the, time, in, urð]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>[factsguide, society, now, motivation]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  \\\n",
       "0      0   \n",
       "1      0   \n",
       "2      0   \n",
       "3      0   \n",
       "4      0   \n",
       "\n",
       "                                                                                                 tweet  \\\n",
       "0  @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.  ...   \n",
       "1  @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.  ...   \n",
       "2                                                                                  bihday your majesty   \n",
       "3               #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦     \n",
       "4                                                               factsguide: society now    #motivation   \n",
       "\n",
       "                                                                                           clean_tweet  \n",
       "0  [user, when, father, is, dysfunctional, and, is, so, selfish, he, drags, his, kids, into, his, d...  \n",
       "1  [user, user, thanks, for, lyft, credit, can, use, cause, they, don, offer, wheelchair, vans, in,...  \n",
       "2                                                                              [bihday, your, majesty]  \n",
       "3                                                   [model, love, take, with, all, the, time, in, urð]  \n",
       "4                                                               [factsguide, society, now, motivation]  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "# clean data by using gensim built in function:\n",
    "tweet_gensim['clean_tweet'] = tweet_gensim ['tweet'].apply(lambda x : gensim.utils.simple_preprocess(x))\n",
    "tweet_gensim.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = train_test_split(tweet_gensim, test_size  = 0.2 , random_state=142)\n",
    "X_train = train['clean_tweet']\n",
    "y_train = train['label']\n",
    "X_test = test['clean_tweet']\n",
    "y_test = test['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 .Vectorization : word2vec embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.Word2Vec ( X_train,   # **train the w2v model only on training set.**IMP\n",
    "                                    vector_size = 10 , #size of the vectors\n",
    "                                    window = 4,   #number of words before and after the keyword to understand context in which the word is used\n",
    "                                    min_count =2)   #the minimum time that a word must be appeared in the text in order to create the word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.07217278, -0.00500214,  1.6541885 , -0.49206886,  0.6383111 ,\n",
       "        0.01688413,  0.60983694,  1.2963771 , -2.1193588 , -2.0665634 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv['motivation'] # get numpy vector of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('inspiration', 0.9965179562568665),\n",
       " ('energy', 0.9937769770622253),\n",
       " ('kevin', 0.9869865775108337),\n",
       " ('bogotadc', 0.9838134050369263),\n",
       " ('joy', 0.9835760593414307),\n",
       " ('meditation', 0.9819909930229187),\n",
       " ('danger', 0.9807791709899902),\n",
       " ('peaceful', 0.9804904460906982),\n",
       " ('gratitude', 0.9795219302177429),\n",
       " ('cretin', 0.9786925911903381)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('motivation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replace all words in each text with the learnt word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smora\\AppData\\Local\\Temp\\ipykernel_8100\\3992832946.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_vect = np.array ([np.array([w2v_model.wv[word] for word in txt if word in vect_words])  #  if word in vect_words. means make sure that model did learn about the word\n",
      "C:\\Users\\smora\\AppData\\Local\\Temp\\ipykernel_8100\\3992832946.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  test_vect = np.array ([np.array([w2v_model.wv[word] for word in txt if word in vect_words])  #  if word in vect_words. means make sure that model did learn about the word\n"
     ]
    }
   ],
   "source": [
    "vect_words = set (w2v_model.wv.index_to_key)  # use index_to_key attribute from the train model which is the list of words.\n",
    "# words represent all the words that word2vec knows about.\n",
    "train_vect = np.array ([np.array([w2v_model.wv[word] for word in txt if word in vect_words])  #  if word in vect_words. means make sure that model did learn about the word\n",
    "                        for txt in X_train])\n",
    "test_vect = np.array ([np.array([w2v_model.wv[word] for word in txt if word in vect_words])  \n",
    "                        for txt in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.91048276e+00, -1.24070573e+00,  1.99192584e+00,\n",
       "         2.72942662e+00,  1.04834354e+00, -5.76628149e-01,\n",
       "         2.67861247e+00, -8.40476871e-01, -4.30134201e+00,\n",
       "         4.77375239e-01],\n",
       "       [ 8.08413625e-01, -6.77194774e-01,  1.18487060e+00,\n",
       "         6.83297098e-01,  1.04146302e+00,  4.59551573e-01,\n",
       "         2.65634346e+00,  1.09223664e+00, -2.27686501e+00,\n",
       "        -4.73695278e-01],\n",
       "       [ 1.80971876e-01, -2.40165472e-01,  7.52116799e-01,\n",
       "        -2.05449149e-01,  5.39051592e-01, -7.89066702e-02,\n",
       "         1.52979743e+00,  4.46467608e-01, -1.61578238e+00,\n",
       "        -5.84985137e-01],\n",
       "       [-1.39852151e-01,  1.19559467e-01,  7.70649076e-01,\n",
       "         5.70386946e-02,  7.90282607e-01, -2.69560307e-01,\n",
       "         2.76214623e+00,  6.18983209e-01, -2.38378453e+00,\n",
       "        -2.36807212e-01],\n",
       "       [-1.09718740e+00,  1.02908635e+00,  4.83179867e-01,\n",
       "         1.01757377e-01,  1.72091818e+00,  5.58452487e-01,\n",
       "         3.97436357e+00,  2.23353362e+00, -3.55532742e+00,\n",
       "        -3.64916176e-01],\n",
       "       [ 1.41695416e+00, -1.50872779e+00,  1.59728765e+00,\n",
       "         1.19638443e+00,  1.87686324e+00, -2.14946938e+00,\n",
       "         3.02624011e+00, -5.40171921e-01, -5.51002598e+00,\n",
       "         2.08844686e+00],\n",
       "       [-1.17419645e-01, -1.24523866e+00,  5.63687682e-01,\n",
       "        -4.53151107e-01,  8.52600992e-01,  9.02138770e-01,\n",
       "         1.33287168e+00,  2.80608273e+00, -2.71903849e+00,\n",
       "        -4.97393817e-01],\n",
       "       [-8.99433255e-01, -2.79042625e+00,  1.54128122e+00,\n",
       "         3.63033563e-01, -1.30956626e+00, -3.44041660e-02,\n",
       "         1.74315631e+00,  2.28465962e+00, -5.20402813e+00,\n",
       "         3.79687369e-01],\n",
       "       [-1.17833364e+00, -7.95703769e-01,  1.10840595e+00,\n",
       "        -2.19021216e-01,  4.37539548e-01,  1.91189444e+00,\n",
       "         1.14175951e+00,  2.14892912e+00, -3.81277728e+00,\n",
       "        -1.87784418e-01],\n",
       "       [ 1.41695416e+00, -1.50872779e+00,  1.59728765e+00,\n",
       "         1.19638443e+00,  1.87686324e+00, -2.14946938e+00,\n",
       "         3.02624011e+00, -5.40171921e-01, -5.51002598e+00,\n",
       "         2.08844686e+00],\n",
       "       [-7.84519613e-01, -1.39839351e+00,  1.34401166e+00,\n",
       "         9.50966552e-02, -1.32263839e+00,  6.31916746e-02,\n",
       "         3.79063749e+00,  1.50937915e+00, -5.28504896e+00,\n",
       "         6.65911555e-01],\n",
       "       [ 1.53780282e+00, -2.37929881e-01, -1.54467389e-01,\n",
       "        -2.45669770e+00,  9.27394390e-01,  1.64300334e+00,\n",
       "         2.66723490e+00,  2.38851309e+00, -4.66469669e+00,\n",
       "        -8.20315540e-01],\n",
       "       [-1.74150646e+00, -3.39577585e-01,  1.82165313e+00,\n",
       "        -2.46503100e-01,  2.86697298e-01, -8.81517529e-01,\n",
       "         4.32350636e+00,  5.16730368e-01, -4.74664593e+00,\n",
       "         3.55406888e-02],\n",
       "       [ 2.16361716e-01, -1.59137595e+00,  1.29151702e+00,\n",
       "         2.61476994e+00,  9.42753732e-01,  1.93494678e+00,\n",
       "         4.63349009e+00,  1.07667267e+00, -4.44867945e+00,\n",
       "         4.14302766e-01],\n",
       "       [-1.24998641e+00,  3.15193027e-01,  2.79944444e+00,\n",
       "        -1.00727665e+00,  9.24660638e-02, -7.67852843e-01,\n",
       "         4.69671202e+00, -2.91896164e-01, -4.33333874e+00,\n",
       "        -6.25659071e-04],\n",
       "       [-5.93507662e-02, -1.42399669e-01,  2.89059997e-01,\n",
       "        -1.16548836e-01, -4.91669588e-03,  9.93488915e-03,\n",
       "         5.41446924e-01,  1.74516276e-01, -6.76055133e-01,\n",
       "        -3.92505005e-02],\n",
       "       [ 5.71686253e-02, -6.15609176e-02,  1.55698866e-01,\n",
       "        -8.46030340e-02,  1.52093321e-01, -4.58181612e-02,\n",
       "        -3.52383107e-02,  1.58024535e-01, -1.39200836e-01,\n",
       "        -5.02179638e-02],\n",
       "       [-3.96160521e-02,  4.90638204e-02,  1.22693487e-01,\n",
       "         1.45818489e-02,  1.39658079e-01, -9.45171900e-03,\n",
       "         1.66002229e-01,  7.62973353e-02, -1.26347706e-01,\n",
       "        -1.50637314e-01],\n",
       "       [-1.17833364e+00, -7.95703769e-01,  1.10840595e+00,\n",
       "        -2.19021216e-01,  4.37539548e-01,  1.91189444e+00,\n",
       "         1.14175951e+00,  2.14892912e+00, -3.81277728e+00,\n",
       "        -1.87784418e-01],\n",
       "       [ 3.48549247e-01, -3.77458662e-01,  2.09443212e+00,\n",
       "        -2.59367198e-01,  7.34674573e-01,  5.12028933e-01,\n",
       "         1.07095674e-01,  1.24193883e+00, -2.21629477e+00,\n",
       "        -2.60626483e+00]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vect[0]  # Thats the array of the arrays. (one array for every word in the text message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg of the word vectors for each sentence, to get a single vector representation with a fixed length.\n",
    "# (Assume to assign zero if the model didnt learn about any of the words in sentence.)\n",
    "avg_train_vect = []\n",
    "for v in train_vect: # v is an array of arrays that we create in previous cell\n",
    "    if v.size:\n",
    "        avg_train_vect.append(v.mean(axis = 0))\n",
    "    else:\n",
    "        avg_train_vect.append(np.zeros(10,dtype = float))\n",
    "\n",
    "avg_test_vect = []\n",
    "for v in test_vect:\n",
    "    if v.size:\n",
    "        avg_test_vect.append(v.mean(axis = 0))\n",
    "    else:\n",
    "        avg_test_vect.append(np.zeros(10,dtype = float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.9593989e-02, -6.7191947e-01,  1.1231573e+00,  1.8920657e-01,\n",
       "        5.6300396e-01,  1.4719795e-01,  2.2952085e+00,  9.3545902e-01,\n",
       "       -3.3669040e+00, -2.5483607e-03], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_train_vect[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 . Fit Ranodom Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf_model =rf.fit(avg_train_vect,y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_model.predict(avg_test_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 .Evaluation the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision : 0.986, recall : 0.166, Accuracy : 0.943\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(y_test,y_pred)\n",
    "\n",
    "recall =recall_score(y_test,y_pred)\n",
    "\n",
    "print ('precision : {}, recall : {}, Accuracy : {}'.format (\n",
    "                                                            round(precision, 3),\n",
    "                                                            round(recall , 3),\n",
    "                                                            round(((y_pred == y_test).sum() / len(y_pred)),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Built in a Basic RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 . Tokenization dataset by keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train_tweet)\n",
    "# integer encode documents\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_tweet )\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test_tweet )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize what was learned\n",
    "#print(t.word_counts)\n",
    "#print(t.document_count)\n",
    "#print(t.word_index)\n",
    "#print(t.word_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 . Pad the sequences with the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq_padd = pad_sequences(X_train_seq,20)\n",
    "X_test_seq_padd = pad_sequences(X_test_seq,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[470, 52, 141, 12, 8, 412, 5096, 3729, 12, 110]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_seq[0]\n",
    "# each integer representing a word in the first text message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  470,\n",
       "         52,  141,   12,    8,  412, 5096, 3729,   12,  110])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_seq_padd[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 . Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 32)          1141120   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 32)                8320      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,150,529\n",
      "Trainable params: 1,150,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.index_word)+1,32))\n",
    "model.add (LSTM (32,dropout = 0, recurrent_dropout = 0 ))\n",
    "model.add (Dense(32, activation ='relu'))\n",
    "model.add (Dense(1, activation ='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics ='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "800/800 [==============================] - 58s 64ms/step - loss: 0.1699 - accuracy: 0.9468 - val_loss: 0.1178 - val_accuracy: 0.9614\n",
      "Epoch 2/10\n",
      "800/800 [==============================] - 51s 64ms/step - loss: 0.0614 - accuracy: 0.9793 - val_loss: 0.1368 - val_accuracy: 0.9643\n",
      "Epoch 3/10\n",
      "800/800 [==============================] - 50s 62ms/step - loss: 0.0272 - accuracy: 0.9910 - val_loss: 0.1373 - val_accuracy: 0.9637\n",
      "Epoch 4/10\n",
      "800/800 [==============================] - 51s 64ms/step - loss: 0.0121 - accuracy: 0.9963 - val_loss: 0.2195 - val_accuracy: 0.9595\n",
      "Epoch 5/10\n",
      "800/800 [==============================] - 51s 64ms/step - loss: 0.0067 - accuracy: 0.9978 - val_loss: 0.2347 - val_accuracy: 0.9609\n",
      "Epoch 6/10\n",
      "800/800 [==============================] - 51s 64ms/step - loss: 0.0047 - accuracy: 0.9986 - val_loss: 0.3146 - val_accuracy: 0.9621\n",
      "Epoch 7/10\n",
      "800/800 [==============================] - 50s 63ms/step - loss: 0.0038 - accuracy: 0.9987 - val_loss: 0.2637 - val_accuracy: 0.9603\n",
      "Epoch 8/10\n",
      "800/800 [==============================] - 52s 65ms/step - loss: 0.0032 - accuracy: 0.9989 - val_loss: 0.3005 - val_accuracy: 0.9587\n",
      "Epoch 9/10\n",
      "800/800 [==============================] - 52s 64ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.3566 - val_accuracy: 0.9568\n",
      "Epoch 10/10\n",
      "800/800 [==============================] - 53s 66ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.3112 - val_accuracy: 0.9578\n"
     ]
    }
   ],
   "source": [
    "history = model.fit (X_train_seq_padd,y_train_tweet,\n",
    "                    batch_size = 32,\n",
    "                    epochs = 10,\n",
    "                    validation_data = (X_test_seq_padd,y_test_tweet) )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
